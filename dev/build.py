import os
from datetime import datetime


def main(urd):

	# Look recursively in this path for files matching
	# (case-insensitive) these extensions.
	path = urd.info.input_directory  # or "path = <abspath_to_my_files>
	extensions = {'.JPG', '.NEF', '.PNG', '.GIF', '.TIFF', '.BMP'}

	# First, "scan" will find all directories and files in the path,
	# and issue one subjob (method "scandir") for each directory
	# containing image files matching the extensions.  These subjobs
	# will create datasets of filenames, hashes of the files'
	# contents, and more.  Finally, it will return a list of all
	# these subjobs. (*)
	#
	# Since this job is the one that detects any modifications to the
	# input path hierarchy, we need it to run unconditionally.  We do
	# this by adding the current timestamp as an input option.
	job = urd.build('scan', path=path, validextensions=extensions, dummy=datetime.now())

	# "dataset_list_to_chain" will read the returned list of subjobs
	# (each job contains one dataset), and write a dataset chain of
	# all datasets. (**)
	job = urd.build('dataset_list_to_chain', source=job.load())

	# The dataset is now re-partitioned in slices based on the hash
	# column.  This linear-time operation will ensure that all entries
	# with the same file hash value ends up in the same slice.
	# Looking for duplicates can then be carried out in parallel,
	# independently in each slice.  Thereafter, all slices are sorted
	# independently, so that rows with the same file hash value will
	# end up next to eachother.  We sort by filename as well in order
	# to ensure reproducibility.
	job = urd.build('dataset_hashpart', source=job, hashlabel='filehash')
	job = urd.build('dataset_sort', source=job, sort_columns=('filehash', 'filename',))

	# The "duplicates" job will find all duplicates in very close to
	# linear time (since the input is sorted).  The resulting report
	# file will be linked in the result directory.
	dup = urd.build('duplicates', source=job)
	dup.link_result('duplicates.txt')


	# (*) Computing file hashes could be time consuming, even though
	#     they are computed in parallel in the "scandir" method.
	#
	#     We use subjobs in order to maximise reuse of existing jobs.
	#
	#     What happens if we change something in the path?
	#
	#     - adding a new directory will cause hashes to be computed for
	#       that directory only.
	#
	#     - adding/removing files from a single directory causes the hashes
	#       for that directory only to be re-computed.
	#
	#     - removing directories does not cause any hash computations.
	#
	#     ANY change(s) in the path will be caught and handled by the
	#     build script when re-executed.
	#
	#
	# (**) Lists of datasets are more efficient when scanning
	#      independent directories, but dataset chains are better when it
	#      comes to processing the data generated by the scanning.
